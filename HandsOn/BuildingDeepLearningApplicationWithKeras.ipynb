{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597895803638",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Note: total_earnings values were scaled by multiplying by 0.0000036968 and adding -0.115913\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv(\"sales_data_training.csv\")\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df = pd.read_csv(\"sales_data_test.csv\")\n",
    "\n",
    "# Data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well.\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "scaled_training = scaler.fit_transform(training_data_df)\n",
    "scaled_testing = scaler.transform(test_data_df)\n",
    "\n",
    "# Print out the adjustment that the scaler applied to the total_earnings column of data\n",
    "print(\"Note: total_earnings values were scaled by multiplying by {:.10f} and adding {:.6f}\".format(scaler.scale_[8], scaler.min_[8]))\n",
    "\n",
    "# Create new pandas DataFrame objects from the scaled data\n",
    "scaled_training_df = pd.DataFrame(scaled_training, columns=training_data_df.columns.values)\n",
    "scaled_testing_df = pd.DataFrame(scaled_testing, columns=test_data_df.columns.values)\n",
    "\n",
    "# Save scaled data dataframes to new CSV files\n",
    "scaled_training_df.to_csv(\"sales_data_training_scaled.csv\", index=False)\n",
    "scaled_testing_df.to_csv(\"sales_data_test_scaled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Keras model using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "training_data_df = pd.read_csv(\"sales_data_training_scaled.csv\")\n",
    "\n",
    "X = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=9, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traing and Evaluation the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n32/32 - 0s - loss: 0.0689\nEpoch 2/50\n32/32 - 0s - loss: 0.0076\nEpoch 3/50\n32/32 - 0s - loss: 0.0022\nEpoch 4/50\n32/32 - 0s - loss: 8.7547e-04\nEpoch 5/50\n32/32 - 0s - loss: 5.0223e-04\nEpoch 6/50\n32/32 - 0s - loss: 3.4950e-04\nEpoch 7/50\n32/32 - 0s - loss: 2.4626e-04\nEpoch 8/50\n32/32 - 0s - loss: 1.8757e-04\nEpoch 9/50\n32/32 - 0s - loss: 1.5020e-04\nEpoch 10/50\n32/32 - 0s - loss: 1.3380e-04\nEpoch 11/50\n32/32 - 0s - loss: 1.0592e-04\nEpoch 12/50\n32/32 - 0s - loss: 9.0066e-05\nEpoch 13/50\n32/32 - 0s - loss: 7.9995e-05\nEpoch 14/50\n32/32 - 0s - loss: 7.1543e-05\nEpoch 15/50\n32/32 - 0s - loss: 6.7761e-05\nEpoch 16/50\n32/32 - 0s - loss: 6.0414e-05\nEpoch 17/50\n32/32 - 0s - loss: 5.2848e-05\nEpoch 18/50\n32/32 - 0s - loss: 4.8212e-05\nEpoch 19/50\n32/32 - 0s - loss: 5.0350e-05\nEpoch 20/50\n32/32 - 0s - loss: 3.9775e-05\nEpoch 21/50\n32/32 - 0s - loss: 4.2375e-05\nEpoch 22/50\n32/32 - 0s - loss: 3.6324e-05\nEpoch 23/50\n32/32 - 0s - loss: 3.2881e-05\nEpoch 24/50\n32/32 - 0s - loss: 3.3314e-05\nEpoch 25/50\n32/32 - 0s - loss: 3.1775e-05\nEpoch 26/50\n32/32 - 0s - loss: 2.9581e-05\nEpoch 27/50\n32/32 - 0s - loss: 2.7329e-05\nEpoch 28/50\n32/32 - 0s - loss: 2.8408e-05\nEpoch 29/50\n32/32 - 0s - loss: 3.3015e-05\nEpoch 30/50\n32/32 - 0s - loss: 2.9885e-05\nEpoch 31/50\n32/32 - 0s - loss: 3.2623e-05\nEpoch 32/50\n32/32 - 0s - loss: 2.3070e-05\nEpoch 33/50\n32/32 - 0s - loss: 2.6261e-05\nEpoch 34/50\n32/32 - 0s - loss: 2.2567e-05\nEpoch 35/50\n32/32 - 0s - loss: 2.2126e-05\nEpoch 36/50\n32/32 - 0s - loss: 2.2180e-05\nEpoch 37/50\n32/32 - 0s - loss: 2.2795e-05\nEpoch 38/50\n32/32 - 0s - loss: 2.7626e-05\nEpoch 39/50\n32/32 - 0s - loss: 2.6150e-05\nEpoch 40/50\n32/32 - 0s - loss: 2.1825e-05\nEpoch 41/50\n32/32 - 0s - loss: 2.7707e-05\nEpoch 42/50\n32/32 - 0s - loss: 5.5553e-05\nEpoch 43/50\n32/32 - 0s - loss: 4.6643e-05\nEpoch 44/50\n32/32 - 0s - loss: 3.3346e-05\nEpoch 45/50\n32/32 - 0s - loss: 2.3717e-05\nEpoch 46/50\n32/32 - 0s - loss: 2.2294e-05\nEpoch 47/50\n32/32 - 0s - loss: 2.0345e-05\nEpoch 48/50\n32/32 - 0s - loss: 2.5884e-05\nEpoch 49/50\n32/32 - 0s - loss: 2.9784e-05\nEpoch 50/50\n32/32 - 0s - loss: 1.7769e-05\nThe mean squared error (MSE) for the test data set is: 7.278862176463008e-05\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "training_data_df = pd.read_csv(\"sales_data_training_scaled.csv\")\n",
    "\n",
    "X = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=9, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=50, shuffle=True, verbose=2)\n",
    "\n",
    "# Load the separate test data set\n",
    "test_data_df = pd.read_csv(\"sales_data_test_scaled.csv\")\n",
    "\n",
    "X_test = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_test = test_data_df[['total_earnings']].values\n",
    "\n",
    "test_error_rate = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"The mean squared error (MSE) for the test data set is: {}\".format(test_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making perdictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Earnings Prediction for Proposed Product - $266108.9631443647\n"
    }
   ],
   "source": [
    "# Load the data we make to use to make a prediction\n",
    "X = pd.read_csv(\"proposed_new_product.csv\").values\n",
    "\n",
    "# Make a prediction with the neural network\n",
    "prediction = model.predict(X)\n",
    "\n",
    "# Grab just the first element of the first prediction (since that's the only have one)\n",
    "prediction = prediction[0][0]\n",
    "\n",
    "# Re-scale the data from the 0-to-1 range back to dollars\n",
    "# These constants are from when the data was originally scaled down to the 0-to-1 range\n",
    "prediction = prediction + 0.1159\n",
    "prediction = prediction / 0.0000036968\n",
    "\n",
    "print(\"Earnings Prediction for Proposed Product - ${}\".format(prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model saved to disk.\n"
    }
   ],
   "source": [
    "# Save the model to disk\n",
    "model.save('trained_model.h5')\n",
    "print(\"Model saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Earnings Prediction for Proposed Product - $266108.9631443647\n"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"trained_model.h5\")\n",
    "\n",
    "X = pd.read_csv(\"proposed_new_product.csv\").values\n",
    "prediction = model.predict(X)\n",
    "\n",
    "# Grab just the first element of the first prediction (since we only have one)\n",
    "prediction = prediction[0][0]\n",
    "\n",
    "# Re-scale the data from the 0-to-1 range back to dollars\n",
    "# These constants are from when the data was originally scaled down to the 0-to-1 range\n",
    "prediction = prediction + 0.1159\n",
    "prediction = prediction / 0.0000036968\n",
    "\n",
    "print(\"Earnings Prediction for Proposed Product - ${}\".format(prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize images with ResNet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n102973440/102967424 [==============================] - 162s 2us/step\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n40960/35363 [==================================] - 0s 9us/step\nThis is an image of:\n - seashore: 0.570416 likelihood\n - lakeside: 0.292265 likelihood\n - dock: 0.082964 likelihood\n - breakwater: 0.037219 likelihood\n - promontory: 0.005919 likelihood\n - catamaran: 0.002877 likelihood\n - sandbar: 0.001334 likelihood\n - trimaran: 0.000754 likelihood\n - pier: 0.000743 likelihood\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import resnet50\n",
    "\n",
    "# Load Keras' ResNet50 model that was pre-trained against the ImageNet database\n",
    "model = resnet50.ResNet50()\n",
    "\n",
    "# Load the image file, resizing it to 224x224 pixels (required by this model)\n",
    "img = image.load_img(\"bay.jpg\", target_size=(224, 224))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "x = image.img_to_array(img) \n",
    "\n",
    "# Add a forth dimension since Keras expects a list of images\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Scale the input image to the range used in the trained network\n",
    "x = resnet50.preprocess_input(x)\n",
    "\n",
    "# Run the image through the deep neural network to make a prediction\n",
    "predictions = model.predict(x)\n",
    "\n",
    "# Look up the names of the predicted classes. Index zero is the results for the first image.\n",
    "predicted_classes = resnet50.decode_predictions(predictions, top=9)\n",
    "\n",
    "print(\"This is an image of:\")\n",
    "\n",
    "for imagenet_id, name, likelihood in predicted_classes[0]:\n",
    "    print(\" - {}: {:2f} likelihood\".format(name, likelihood))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Keras logs in TensorFlow format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\nWARNING:tensorflow:From C:\\Users\\Gurjant\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\nInstructions for updating:\nuse `tf.profiler.experimental.stop` instead.\nWARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.1015s). Check your callbacks.\n32/32 - 0s - loss: 0.0310\nEpoch 2/50\n32/32 - 0s - loss: 0.0037\nEpoch 3/50\n32/32 - 0s - loss: 0.0011\nEpoch 4/50\n32/32 - 0s - loss: 5.6026e-04\nEpoch 5/50\n32/32 - 0s - loss: 3.1208e-04\nEpoch 6/50\n32/32 - 0s - loss: 2.1780e-04\nEpoch 7/50\n32/32 - 0s - loss: 1.4152e-04\nEpoch 8/50\n32/32 - 0s - loss: 1.3207e-04\nEpoch 9/50\n32/32 - 0s - loss: 1.0002e-04\nEpoch 10/50\n32/32 - 0s - loss: 7.6446e-05\nEpoch 11/50\n32/32 - 0s - loss: 6.3419e-05\nEpoch 12/50\n32/32 - 0s - loss: 6.2794e-05\nEpoch 13/50\n32/32 - 0s - loss: 5.6360e-05\nEpoch 14/50\n32/32 - 0s - loss: 5.2891e-05\nEpoch 15/50\n32/32 - 0s - loss: 4.4200e-05\nEpoch 16/50\n32/32 - 0s - loss: 5.2062e-05\nEpoch 17/50\n32/32 - 0s - loss: 3.3509e-05\nEpoch 18/50\n32/32 - 0s - loss: 3.4012e-05\nEpoch 19/50\n32/32 - 0s - loss: 2.9005e-05\nEpoch 20/50\n32/32 - 0s - loss: 3.3826e-05\nEpoch 21/50\n32/32 - 0s - loss: 3.3222e-05\nEpoch 22/50\n32/32 - 0s - loss: 3.1733e-05\nEpoch 23/50\n32/32 - 0s - loss: 4.5775e-05\nEpoch 24/50\n32/32 - 0s - loss: 3.1074e-05\nEpoch 25/50\n32/32 - 0s - loss: 3.7850e-05\nEpoch 26/50\n32/32 - 0s - loss: 3.7553e-05\nEpoch 27/50\n32/32 - 0s - loss: 2.0359e-05\nEpoch 28/50\n32/32 - 0s - loss: 2.7363e-05\nEpoch 29/50\n32/32 - 0s - loss: 2.1968e-05\nEpoch 30/50\n32/32 - 0s - loss: 2.3831e-05\nEpoch 31/50\n32/32 - 0s - loss: 2.6808e-05\nEpoch 32/50\n32/32 - 0s - loss: 2.0152e-05\nEpoch 33/50\n32/32 - 0s - loss: 2.3869e-05\nEpoch 34/50\n32/32 - 0s - loss: 2.9929e-05\nEpoch 35/50\n32/32 - 0s - loss: 2.3580e-05\nEpoch 36/50\n32/32 - 0s - loss: 2.1156e-05\nEpoch 37/50\n32/32 - 0s - loss: 2.7996e-05\nEpoch 38/50\n32/32 - 0s - loss: 2.6698e-05\nEpoch 39/50\n32/32 - 0s - loss: 3.3771e-05\nEpoch 40/50\n32/32 - 0s - loss: 2.8860e-05\nEpoch 41/50\n32/32 - 0s - loss: 2.9019e-05\nEpoch 42/50\n32/32 - 0s - loss: 2.8519e-05\nEpoch 43/50\n32/32 - 0s - loss: 2.3492e-05\nEpoch 44/50\n32/32 - 0s - loss: 2.1033e-05\nEpoch 45/50\n32/32 - 0s - loss: 2.6927e-05\nEpoch 46/50\n32/32 - 0s - loss: 2.4974e-05\nEpoch 47/50\n32/32 - 0s - loss: 2.1681e-05\nEpoch 48/50\n32/32 - 0s - loss: 2.4363e-05\nEpoch 49/50\n32/32 - 0s - loss: 1.8180e-05\nEpoch 50/50\n32/32 - 0s - loss: 2.0843e-05\nThe mean squared error (MSE) for the test data set is: 7.893514703027904e-05\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "training_data_df = pd.read_csv(\"sales_data_training_scaled.csv\")\n",
    "\n",
    "X = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=9, activation='relu', name='layer_1'))\n",
    "model.add(Dense(100, activation='relu', name='layer_2'))\n",
    "model.add(Dense(50, activation='relu', name='layer_3'))\n",
    "model.add(Dense(1, activation='linear', name='output_layer'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Create a TensorBoard logger\n",
    "logger = keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs\",\n",
    "    write_graph=True,\n",
    "    histogram_freq=5,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    epochs=50,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[logger]\n",
    ")\n",
    "\n",
    "# Load the separate test data set\n",
    "test_data_df = pd.read_csv(\"sales_data_test_scaled.csv\")\n",
    "\n",
    "X_test = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_test = test_data_df[['total_earnings']].values\n",
    "\n",
    "test_error_rate = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"The mean squared error (MSE) for the test data set is: {}\".format(test_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\nWARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.5055s). Check your callbacks.\n32/32 - 1s - loss: 0.0409\nEpoch 2/50\n32/32 - 0s - loss: 0.0270\nEpoch 3/50\n32/32 - 0s - loss: 0.0236\nEpoch 4/50\n32/32 - 0s - loss: 0.0206\nEpoch 5/50\n32/32 - 0s - loss: 0.0176\nEpoch 6/50\n32/32 - 0s - loss: 0.0154\nEpoch 7/50\n32/32 - 0s - loss: 0.0136\nEpoch 8/50\n32/32 - 0s - loss: 0.0113\nEpoch 9/50\n32/32 - 0s - loss: 0.0098\nEpoch 10/50\n32/32 - 0s - loss: 0.0081\nEpoch 11/50\n32/32 - 0s - loss: 0.0066\nEpoch 12/50\n32/32 - 0s - loss: 0.0050\nEpoch 13/50\n32/32 - 0s - loss: 0.0039\nEpoch 14/50\n32/32 - 0s - loss: 0.0033\nEpoch 15/50\n32/32 - 0s - loss: 0.0023\nEpoch 16/50\n32/32 - 0s - loss: 0.0018\nEpoch 17/50\n32/32 - 0s - loss: 0.0014\nEpoch 18/50\n32/32 - 0s - loss: 0.0010\nEpoch 19/50\n32/32 - 0s - loss: 7.7143e-04\nEpoch 20/50\n32/32 - 0s - loss: 6.4499e-04\nEpoch 21/50\n32/32 - 0s - loss: 5.3022e-04\nEpoch 22/50\n32/32 - 0s - loss: 4.6682e-04\nEpoch 23/50\n32/32 - 0s - loss: 4.1241e-04\nEpoch 24/50\n32/32 - 0s - loss: 3.7466e-04\nEpoch 25/50\n32/32 - 0s - loss: 3.3186e-04\nEpoch 26/50\n32/32 - 0s - loss: 3.0686e-04\nEpoch 27/50\n32/32 - 0s - loss: 2.6049e-04\nEpoch 28/50\n32/32 - 0s - loss: 2.4931e-04\nEpoch 29/50\n32/32 - 0s - loss: 2.2345e-04\nEpoch 30/50\n32/32 - 0s - loss: 2.0405e-04\nEpoch 31/50\n32/32 - 0s - loss: 2.0855e-04\nEpoch 32/50\n32/32 - 0s - loss: 2.1479e-04\nEpoch 33/50\n32/32 - 0s - loss: 1.9628e-04\nEpoch 34/50\n32/32 - 0s - loss: 1.8296e-04\nEpoch 35/50\n32/32 - 0s - loss: 1.7640e-04\nEpoch 36/50\n32/32 - 0s - loss: 1.6905e-04\nEpoch 37/50\n32/32 - 0s - loss: 1.7469e-04\nEpoch 38/50\n32/32 - 0s - loss: 1.5015e-04\nEpoch 39/50\n32/32 - 0s - loss: 1.5071e-04\nEpoch 40/50\n32/32 - 0s - loss: 1.4222e-04\nEpoch 41/50\n32/32 - 0s - loss: 1.2779e-04\nEpoch 42/50\n32/32 - 0s - loss: 1.2785e-04\nEpoch 43/50\n32/32 - 0s - loss: 1.4134e-04\nEpoch 44/50\n32/32 - 0s - loss: 1.2358e-04\nEpoch 45/50\n32/32 - 0s - loss: 1.2789e-04\nEpoch 46/50\n32/32 - 0s - loss: 1.2626e-04\nEpoch 47/50\n32/32 - 0s - loss: 1.0155e-04\nEpoch 48/50\n32/32 - 0s - loss: 9.3668e-05\nEpoch 49/50\n32/32 - 0s - loss: 9.7391e-05\nEpoch 50/50\n32/32 - 0s - loss: 1.0106e-04\nThe mean squared error (MSE) for the test data set is: 0.00016655582294333726\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "RUN_NAME = \"run 5 with 5 nodes\"\n",
    "\n",
    "training_data_df = pd.read_csv(\"sales_data_training_scaled.csv\")\n",
    "\n",
    "X = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=9, activation='relu', name='layer_1'))\n",
    "model.add(Dense(100, activation='relu', name='layer_2'))\n",
    "model.add(Dense(50, activation='relu', name='layer_3'))\n",
    "model.add(Dense(1, activation='linear', name='output_layer'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Create a TensorBoard logger\n",
    "logger = keras.callbacks.TensorBoard(\n",
    "    log_dir='logs/{}'.format(RUN_NAME),\n",
    "    histogram_freq=5,\n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    epochs=50,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[logger]\n",
    ")\n",
    "\n",
    "# Load the separate test data set\n",
    "test_data_df = pd.read_csv(\"sales_data_test_scaled.csv\")\n",
    "\n",
    "X_test = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_test = test_data_df[['total_earnings']].values\n",
    "\n",
    "test_error_rate = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"The mean squared error (MSE) for the test data set is: {}\".format(test_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Google Cloud-compatible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 1000 samples\nEpoch 1/50\nWARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0370s). Check your callbacks.\n1000/1000 - 0s - loss: 0.0149\nEpoch 2/50\n1000/1000 - 0s - loss: 0.0015\nEpoch 3/50\n1000/1000 - 0s - loss: 6.8470e-04\nEpoch 4/50\n1000/1000 - 0s - loss: 3.8683e-04\nEpoch 5/50\n1000/1000 - 0s - loss: 2.1991e-04\nEpoch 6/50\n1000/1000 - 0s - loss: 1.5910e-04\nEpoch 7/50\n1000/1000 - 0s - loss: 1.1701e-04\nEpoch 8/50\n1000/1000 - 0s - loss: 9.8119e-05\nEpoch 9/50\n1000/1000 - 0s - loss: 1.0681e-04\nEpoch 10/50\n1000/1000 - 0s - loss: 7.8834e-05\nEpoch 11/50\n1000/1000 - 0s - loss: 5.8388e-05\nEpoch 12/50\n1000/1000 - 0s - loss: 5.4022e-05\nEpoch 13/50\n1000/1000 - 0s - loss: 4.6347e-05\nEpoch 14/50\n1000/1000 - 0s - loss: 4.7735e-05\nEpoch 15/50\n1000/1000 - 0s - loss: 4.4843e-05\nEpoch 16/50\n1000/1000 - 0s - loss: 5.0076e-05\nEpoch 17/50\n1000/1000 - 0s - loss: 4.6011e-05\nEpoch 18/50\n1000/1000 - 0s - loss: 4.0474e-05\nEpoch 19/50\n1000/1000 - 0s - loss: 3.5247e-05\nEpoch 20/50\n1000/1000 - 0s - loss: 2.9812e-05\nEpoch 21/50\n1000/1000 - 0s - loss: 3.2258e-05\nEpoch 22/50\n1000/1000 - 0s - loss: 2.8793e-05\nEpoch 23/50\n1000/1000 - 0s - loss: 2.8068e-05\nEpoch 24/50\n1000/1000 - 0s - loss: 2.7085e-05\nEpoch 25/50\n1000/1000 - 0s - loss: 3.1862e-05\nEpoch 26/50\n1000/1000 - 0s - loss: 3.0679e-05\nEpoch 27/50\n1000/1000 - 0s - loss: 3.3961e-05\nEpoch 28/50\n1000/1000 - 0s - loss: 2.6651e-05\nEpoch 29/50\n1000/1000 - 0s - loss: 2.8616e-05\nEpoch 30/50\n1000/1000 - 0s - loss: 3.3173e-05\nEpoch 31/50\n1000/1000 - 0s - loss: 2.9664e-05\nEpoch 32/50\n1000/1000 - 0s - loss: 3.3259e-05\nEpoch 33/50\n1000/1000 - 0s - loss: 5.7223e-05\nEpoch 34/50\n1000/1000 - 0s - loss: 5.6777e-05\nEpoch 35/50\n1000/1000 - 0s - loss: 3.5392e-05\nEpoch 36/50\n1000/1000 - 0s - loss: 4.2509e-05\nEpoch 37/50\n1000/1000 - 0s - loss: 3.8305e-05\nEpoch 38/50\n1000/1000 - 0s - loss: 3.9608e-05\nEpoch 39/50\n1000/1000 - 0s - loss: 3.2003e-05\nEpoch 40/50\n1000/1000 - 0s - loss: 3.0283e-05\nEpoch 41/50\n1000/1000 - 0s - loss: 2.6545e-05\nEpoch 42/50\n1000/1000 - 0s - loss: 2.8903e-05\nEpoch 43/50\n1000/1000 - 0s - loss: 2.4604e-05\nEpoch 44/50\n1000/1000 - 0s - loss: 2.6224e-05\nEpoch 45/50\n1000/1000 - 0s - loss: 3.1857e-05\nEpoch 46/50\n1000/1000 - 0s - loss: 2.4886e-05\nEpoch 47/50\n1000/1000 - 0s - loss: 3.1174e-05\nEpoch 48/50\n1000/1000 - 0s - loss: 2.1614e-05\nEpoch 49/50\n1000/1000 - 0s - loss: 4.3228e-05\nEpoch 50/50\n1000/1000 - 0s - loss: 2.7678e-05\nThe mean squared error (MSE) for the test data set is: 7.248318288475276e-05\n"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-18a633d6dc39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mmodel_builder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuilder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"exported_model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "import tensorflow as tf\n",
    "\n",
    "training_data_df = pd.read_csv(\"sales_data_training_scaled.csv\")\n",
    "\n",
    "X = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=9, activation='relu', name='layer_1'))\n",
    "model.add(Dense(100, activation='relu', name='layer_2'))\n",
    "model.add(Dense(50, activation='relu', name='layer_3'))\n",
    "model.add(Dense(1, activation='linear', name='output_layer'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Create a TensorBoard logger\n",
    "logger = keras.callbacks.TensorBoard(\n",
    "    log_dir='logs',\n",
    "    histogram_freq=5,\n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    epochs=50,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[logger]\n",
    ")\n",
    "\n",
    "# Load the separate test data set\n",
    "test_data_df = pd.read_csv(\"sales_data_test_scaled.csv\")\n",
    "\n",
    "X_test = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_test = test_data_df[['total_earnings']].values\n",
    "\n",
    "test_error_rate = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"The mean squared error (MSE) for the test data set is: {}\".format(test_error_rate))\n",
    "\n",
    "model_builder = tf.compat.v1.saved_model.Builder(\"exported_model\")\n",
    "\n",
    "if tf.executing_eagerly():\n",
    "   tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "inputs = {\n",
    "    'input': tf.compat.v1.saved_model.utils.build_tensor_info(model.input)\n",
    "}\n",
    "outputs = {\n",
    "    'earnings': tf.compat.v1.saved_model.utils.build_tensor_info(model.output)\n",
    "}\n",
    "\n",
    "signature_def = tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    method_name=tf.compat.v1.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    ")\n",
    "\n",
    "model_builder.add_meta_graph_and_variables(\n",
    "    tf.compat.v1.keras.backend.get_session(),\n",
    "    tags=[tf.compat.v1.saved_model.tag_constants.SERVING],\n",
    "    signature_def_map={\n",
    "        tf.compat.v1.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def\n",
    "    }\n",
    ")\n",
    "\n",
    "model_builder.save()\n"
   ]
  }
 ]
}